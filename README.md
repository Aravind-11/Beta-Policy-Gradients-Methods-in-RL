# Abstract
### Recently, reinforcement learning with deep neu- ral networks has achieved great success in chal- lenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaus- sian distribution is used as the stochastic policy. In this work, we propose to use the Beta dis- tribution as an alternative and analyze the bias and variance of the policy gradients of both poli- cies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimiza- tion (TRPO) and actor critic with experience re- play (ACER), the state-of-the-art on- and off- policy stochastic methods respectively, on Ope- nAI Gym’s and MuJoCo’s continuous control en- vironments.
